{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wahoo/.pyenv/versions/3.13.2/envs/ootp-analytics/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.FieldingModel import FieldingModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "league =  'BZ-Dev-Test'\n",
    "season_start = 2025\n",
    "season_end = 2075\n",
    "ip_limit = 10\n",
    "epochs = 10000\n",
    "positions = [9,8,7,6,5,4,3,2]\n",
    "ratings_type = \"Standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model for position 9\n",
      "Empty DataFrame\n",
      "Columns: [WT, SPE, OF RNG, OF ARM, OF ERR, runsPAdjSeason]\n",
      "Index: []\n",
      "(7980, 6)\n",
      "Epoch [2000/10000], Best Loss: 30.4076042\n",
      "Epoch [4000/10000], Best Loss: 29.7343731\n",
      "Epoch [6000/10000], Best Loss: 29.3501530\n",
      "Epoch [8000/10000], Best Loss: 29.0606060\n",
      "Epoch [10000/10000], Best Loss: 28.8345680\n",
      "Epoch [9999/10000], Loss: 28.8724022\n",
      "Test Mean Squared Error: 31.7002850\n",
      "   mean_abs_shap  stdev_abs_shap    name\n",
      "2      11.218267        6.712267  OF RNG\n",
      "1       0.838036        0.759767     SPE\n",
      "0       0.671309        1.020230      WT\n",
      "3       0.566145        0.579860  OF ARM\n",
      "4       0.358695        0.425894  OF ERR\n",
      "       WT  SPE  OF RNG  OF ARM  OF ERR  Predictions     ID\n",
      "330   200   60      70      70      75    22.005119  66573\n",
      "1301  206   70      65      70      65    19.464388  81609\n",
      "998   182   75      70      60      65    19.369440  74459\n",
      "1191  202   75      65      65      70    19.293200  76254\n",
      "1282  201   70      65      65      65    17.909897  79249\n",
      "951   193   75      65      70      60    17.676640  74017\n",
      "1215  205   60      65      75      55    17.406185  76556\n",
      "553   185   70      65      70      60    17.140083  70087\n",
      "451   199   65      65      70      60    16.893995  68308\n",
      "974   201   60      65      65      65    16.843214  74328\n",
      "592   202   70      65      65      55    16.654766  70495\n",
      "1185  186   70      65      65      65    16.611582  76238\n",
      "1052  200   65      65      60      60    16.002081  74946\n",
      "546   201   65      65      65      55    15.962703  69932\n",
      "1276  180   60      65      65      60    15.892497  78230\n",
      "158   200   65      65      70      50    15.785272  63811\n",
      "740   196   65      65      60      60    15.664393  71758\n",
      "1210  183   70      65      60      50    15.624439  76485\n",
      "287   201   60      65      70      50    15.279362  66090\n",
      "687   198   65      65      60      55    15.254677  71360\n",
      "1135  193   65      65      60      50    14.791629  75612\n",
      "724   201   70      65      45      45    14.507601  71667\n",
      "1099  195   60      65      50      55    14.284565  75400\n",
      "708   200   60      60      65      65    13.571775  71599\n",
      "942   203   55      60      65      60    13.265749  73986\n",
      "608   204   45      60      60      60    13.187230  70546\n",
      "434   199   65      60      65      75    12.944938  68269\n",
      "632   201   70      60      60      65    12.839332  70873\n",
      "312   200   70      60      60      55    12.775859  66400\n",
      "831   201   60      60      70      60    12.632282  72977\n",
      "1254  200   45      60      75      65    12.627346  77885\n",
      "1021  199   65      60      55      60    12.595434  74866\n",
      "1064  201   70      60      70      55    12.593685  75276\n",
      "643   202   65      60      65      55    12.577251  70893\n",
      "1297  202   65      60      60      75    12.445135  80555\n",
      "1068  200   70      60      55      55    12.400236  75300\n",
      "477   201   50      60      70      55    12.340195  68911\n",
      "640   200   60      60      55      55    12.237959  70889\n",
      "1014  203   70      60      65      60    12.189214  74790\n",
      "492   202   55      60      60      50    12.079363  69075\n",
      "788   197   50      60      75      45    12.059257  72320\n",
      "590   202   55      60      55      50    12.044127  70445\n",
      "615   203   65      60      50      50    11.923659  70574\n",
      "537   204   70      60      70      55    11.535957  69637\n",
      "712   189   55      60      60      55    11.081517  71626\n",
      "983   194   55      60      55      50    10.929218  74356\n",
      "582   188   60      60      75      50    10.635191  70360\n",
      "929   186   60      60      75      50     9.916168  73646\n",
      "626   189   65      60      65      75     9.086686  70865\n",
      "1159  183   70      60      55      50     8.650284  75840\n",
      "\n",
      "Training model for position 8\n",
      "Empty DataFrame\n",
      "Columns: [WT, SPE, OF RNG, OF ARM, OF ERR, runsPAdjSeason]\n",
      "Index: []\n",
      "(7693, 6)\n",
      "Epoch [2000/10000], Best Loss: 34.2063980\n",
      "Epoch [4000/10000], Best Loss: 33.1180115\n",
      "Epoch [6000/10000], Best Loss: 32.3851624\n",
      "Epoch [8000/10000], Best Loss: 31.7336845\n",
      "Epoch [10000/10000], Best Loss: 31.2765579\n",
      "Epoch [9999/10000], Loss: 31.2788105\n",
      "Test Mean Squared Error: 35.3582611\n",
      "   mean_abs_shap  stdev_abs_shap    name\n",
      "2      20.775342        9.604536  OF RNG\n",
      "0       1.025725        1.165386      WT\n",
      "1       0.775537        0.832402     SPE\n",
      "4       0.722805        0.896813  OF ERR\n",
      "3       0.667177        0.764979  OF ARM\n",
      "       WT  SPE  OF RNG  OF ARM  OF ERR  Predictions     ID\n",
      "591   201   75      75      70      65    26.770321  70472\n",
      "1245  180   70      75      60      65    26.071678  77545\n",
      "1177  190   75      75      50      55    25.350830  76209\n",
      "1054  186   70      75      75      60    24.476889  74957\n",
      "1293  163   80      70      65      40    24.371645  80203\n",
      "1294  199   60      75      55      65    23.908716  80542\n",
      "1238  207   70      70      60      70    22.751751  76895\n",
      "997   182   75      70      60      65    21.713778  74459\n",
      "329   200   60      70      70      75    20.691542  66573\n",
      "930   205   60      70      55      55    19.622007  73655\n",
      "1216  200   65      70      65      60    19.319357  76567\n",
      "943   201   60      70      60      60    18.671564  73987\n",
      "365   203   70      65      70      80    18.305435  66981\n",
      "896   203   70      65      65      75    17.172407  73583\n",
      "1285  178   70      65      60      65    16.975319  79329\n",
      "1275  180   60      65      65      60    16.042259  78230\n",
      "516   185   45      65      80      60    15.933348  69290\n",
      "1190  202   75      65      65      70    15.604487  76254\n",
      "1244  162   70      65      60      50    15.260207  77492\n",
      "188   199   65      65      60      70    14.996836  64349\n",
      "1184  186   70      65      65      65    14.940187  76238\n",
      "1281  201   70      65      65      65    14.603958  79249\n",
      "950   193   75      65      70      60    14.399779  74017\n",
      "662   191   60      65      70      45    14.365011  70959\n",
      "411   185   70      65      50      65    14.323904  67887\n",
      "723   201   70      65      45      45    14.254875  71667\n",
      "768   203   65      65      50      70    14.102394  72287\n",
      "1271  188   45      65      65      50    14.076225  78220\n",
      "1147  193   60      65      55      45    13.924849  75739\n",
      "1209  183   70      65      60      50    13.781034  76485\n",
      "973   201   60      65      65      65    13.756761  74328\n",
      "1051  200   65      65      60      60    13.742780  74946\n",
      "686   198   65      65      60      55    13.667470  71360\n",
      "1134  193   65      65      60      50    13.648915  75612\n",
      "1300  206   70      65      70      65    13.640451  81609\n",
      "273   202   70      65      55      60    13.519730  65644\n",
      "739   196   65      65      60      60    13.501413  71758\n",
      "1214  205   60      65      75      55    13.406166  76556\n",
      "286   201   60      65      70      50    13.326475  66090\n",
      "1140  192   65      65      55      60    13.233113  75656\n",
      "226   201   60      65      65      55    13.183102  65049\n",
      "800   200   60      65      70      65    13.166571  72684\n",
      "1057  202   70      65      60      55    13.106650  74972\n",
      "157   200   65      65      70      50    13.090458  63811\n",
      "1023  195   55      65      50      50    13.049993  74877\n",
      "850   195   65      65      55      60    12.963209  73066\n",
      "545   201   65      65      65      55    12.952085  69932\n",
      "415   202   55      65      60      55    12.887914  68203\n",
      "1046  201   70      65      65      55    12.810285  74935\n",
      "1098  195   60      65      50      55    12.763502  75400\n",
      "\n",
      "Training model for position 7\n",
      "Empty DataFrame\n",
      "Columns: [WT, SPE, OF RNG, OF ARM, OF ERR, runsPAdjSeason]\n",
      "Index: []\n",
      "(8152, 6)\n",
      "Epoch [2000/10000], Best Loss: 28.8250713\n",
      "Epoch [4000/10000], Best Loss: 28.2786884\n",
      "Epoch [6000/10000], Best Loss: 27.8803368\n",
      "Epoch [8000/10000], Best Loss: 27.6009045\n",
      "Epoch [10000/10000], Best Loss: 27.3735676\n",
      "Epoch [9999/10000], Loss: 27.4239311\n",
      "Test Mean Squared Error: 29.4889393\n",
      "   mean_abs_shap  stdev_abs_shap    name\n",
      "2      10.620799        6.555711  OF RNG\n",
      "1       0.735731        0.650675     SPE\n",
      "4       0.575761        0.551983  OF ERR\n",
      "3       0.541247        0.590318  OF ARM\n",
      "0       0.510241        0.627025      WT\n",
      "       WT  SPE  OF RNG  OF ARM  OF ERR  Predictions     ID\n",
      "410   185   70      65      50      65    18.667068  67887\n",
      "1274  180   60      65      65      60    18.519865  78230\n",
      "1208  183   70      65      60      50    15.395111  76485\n",
      "1139  192   65      65      55      60    15.032294  75656\n",
      "722   201   70      65      45      45    14.882606  71667\n",
      "767   203   65      65      50      70    14.334605  72287\n",
      "849   195   65      65      55      60    14.302470  73066\n",
      "1097  195   60      65      50      55    14.033605  75400\n",
      "1189  202   75      65      65      70    13.696652  76254\n",
      "1270  188   45      65      65      50    13.385777  78220\n",
      "661   191   60      65      70      45    13.190808  70959\n",
      "1022  195   55      65      50      50    13.166842  74877\n",
      "1280  201   70      65      65      65    13.132710  79249\n",
      "1050  200   65      65      60      60    12.986085  74946\n",
      "156   200   65      65      70      50    12.476827  63811\n",
      "645   202   60      60      40      55    12.446052  70912\n",
      "225   201   60      65      65      55    12.170035  65049\n",
      "624   189   65      60      65      75    11.995553  70865\n",
      "862   200   70      60      45      50    11.747229  73190\n",
      "613   203   65      60      50      50    10.756811  70574\n",
      "143   201   65      60      50      50    10.731884  63481\n",
      "607   204   45      60      60      60    10.427522  70546\n",
      "1019  199   65      60      55      60    10.219995  74866\n",
      "1066  200   70      60      55      55    10.066468  75300\n",
      "786   197   50      60      75      45    10.033454  72320\n",
      "588   202   55      60      55      50     9.958741  70445\n",
      "981   194   55      60      55      50     9.881518  74356\n",
      "745   201   60      60      55      60     9.858524  71767\n",
      "490   202   55      60      60      50     9.596975  69075\n",
      "694   200   45      60      60      55     9.571358  71397\n",
      "638   200   60      60      55      55     9.556780  70889\n",
      "941   203   55      60      65      60     9.323567  73986\n",
      "1231  199   55      60      65      55     9.227656  76817\n",
      "475   201   50      60      70      55     9.208249  68911\n",
      "310   200   70      60      60      55     9.033833  66400\n",
      "641   202   65      60      65      55     8.997854  70893\n",
      "1136  199   70      60      60      60     8.692460  75641\n",
      "647   199   60      60      70      75     8.392467  70923\n",
      "1295  202   65      60      60      75     8.365120  80555\n",
      "829   201   60      60      70      60     8.270549  72977\n",
      "432   199   65      60      65      75     8.058702  68269\n",
      "314   177   60      60      65      50     7.958893  66423\n",
      "1157  183   70      60      55      50     7.812030  75840\n",
      "1062  201   70      60      70      55     7.358767  75276\n",
      "1012  203   70      60      65      60     7.284181  74790\n",
      "183   196   70      55      40      60     5.698324  64311\n",
      "900   203   40      55      60      40     5.487098  73602\n",
      "970   176   40      55      65      65     4.174752  74310\n",
      "663   200   60      55      45      60     3.121451  70966\n",
      "508   193   65      55      50      70     2.990563  69243\n",
      "\n",
      "Training model for position 6\n",
      "Empty DataFrame\n",
      "Columns: [WT, SPE, IF RNG, IF ARM, TDP, IF ERR, runsPAdjSeason]\n",
      "Index: []\n",
      "(7258, 7)\n",
      "Epoch [2000/10000], Best Loss: 35.7775459\n",
      "Epoch [4000/10000], Best Loss: 34.6041565\n",
      "Epoch [6000/10000], Best Loss: 33.8095818\n",
      "Epoch [8000/10000], Best Loss: 33.1231003\n",
      "Epoch [10000/10000], Best Loss: 32.6610107\n",
      "Epoch [9999/10000], Loss: 32.7050743\n",
      "Test Mean Squared Error: 37.4458427\n",
      "   mean_abs_shap  stdev_abs_shap    name\n",
      "2      16.421317        8.865043  IF RNG\n",
      "3       7.315692        5.297141  IF ARM\n",
      "5       2.374153        2.407006  IF ERR\n",
      "4       1.410630        1.781989     TDP\n",
      "0       1.075620        0.940078      WT\n",
      "1       0.936944        1.028018     SPE\n",
      "       WT  SPE  IF RNG  IF ARM  TDP  IF ERR  Predictions     ID\n",
      "986   195   40      75      80   80      80    33.964775  74403\n",
      "1121  194   60      75      80   80      80    33.052311  75546\n",
      "1171  195   50      75      80   80      80    33.021702  76094\n",
      "431   198   80      80      75   75      70    31.619160  68268\n",
      "1155  163   50      75      70   80      80    31.062193  75826\n",
      "234   182   35      75      65   80      80    29.432106  65098\n",
      "1288  200   55      75      80   75      65    27.597572  79356\n",
      "1278  185   50      75      70   70      70    26.319668  78930\n",
      "254   200   65      75      75   70      75    26.167557  65592\n",
      "822   185   65      70      80   70      70    25.414997  72869\n",
      "334   203   35      75      70   70      70    25.132616  66581\n",
      "600   197   30      70      75   60      65    23.336634  70529\n",
      "1163  167   75      75      65   70      65    23.150700  75909\n",
      "507   184   65      75      65   60      65    22.530514  69208\n",
      "1256  182   70      70      75   65      70    21.241537  77891\n",
      "283   188   40      75      70   60      65    21.232550  65849\n",
      "698   169   45      75      60   70      70    20.569950  71451\n",
      "341   196   45      65      80   80      75    20.408627  66907\n",
      "528   200   40      65      80   70      75    20.222795  69592\n",
      "1259  202   40      75      65   60      70    20.137119  77905\n",
      "920   192   55      75      65   65      60    20.078669  73635\n",
      "1146  177   70      70      75   60      45    19.418812  75688\n",
      "700   201   35      65      80   70      65    19.298851  71469\n",
      "325   202   75      65      80   60      65    17.737141  66536\n",
      "711   200   60      70      70   75      65    17.103771  71601\n",
      "703   201   70      70      70   65      65    16.164671  71526\n",
      "999   175   65      70      70   65      55    15.997000  74508\n",
      "861   186   65      70      65   60      65    13.847556  73175\n",
      "636   200   65      65      75   60      65    12.785864  70874\n",
      "856   189   65      65      75   65      55    12.585066  73099\n",
      "465   196   60      65      75   60      65    12.578159  68711\n",
      "388   189   60      65      75   60      75    12.495054  67440\n",
      "1123  202   45      65      75   65      65    12.316275  75555\n",
      "996   200   45      65      75   65      60    12.174120  74437\n",
      "1007  200   65      65      75   65      60    11.511833  74586\n",
      "499   199   45      65      70   80      80    10.516655  69126\n",
      "367   196   70      70      60   65      65     9.964127  66985\n",
      "1202  198   70      70      60   60      70     9.887660  76274\n",
      "734   204   60      65      75   60      60     9.837354  71723\n",
      "744   193   30      65      70   70      60     9.764354  71762\n",
      "378   201   45      65      70   65      75     9.423143  67362\n",
      "460   197   55      65      70   65      70     9.139686  68637\n",
      "370   170   60      65      70   65      65     8.966729  66994\n",
      "568   196   45      60      80   75      75     7.691383  70203\n",
      "419   201   60      65      70   65      70     5.725015  68214\n",
      "793   200   65      65      70   55      50     4.841779  72568\n",
      "577   184   50      60      80   65      60     3.830538  70332\n",
      "308   201   55      65      70   55      65     3.668333  66389\n",
      "885   201   70      60      80   45      50     3.393943  73552\n",
      "121   186   45      60      75   60      70     1.447165  63017\n",
      "\n",
      "Training model for position 5\n",
      "Empty DataFrame\n",
      "Columns: [WT, SPE, IF RNG, IF ARM, TDP, IF ERR, runsPAdjSeason]\n",
      "Index: []\n",
      "(6785, 7)\n",
      "Epoch [2000/10000], Best Loss: 20.3208809\n",
      "Epoch [4000/10000], Best Loss: 19.9013042\n",
      "Epoch [6000/10000], Best Loss: 19.6195526\n",
      "Epoch [8000/10000], Best Loss: 19.4154167\n",
      "Epoch [10000/10000], Best Loss: 19.2196636\n",
      "Epoch [9999/10000], Loss: 19.2248497\n",
      "Test Mean Squared Error: 21.0773849\n",
      "   mean_abs_shap  stdev_abs_shap    name\n",
      "3       4.463783        3.064864  IF ARM\n",
      "2       3.032298        2.065108  IF RNG\n",
      "5       2.034482        1.551681  IF ERR\n",
      "4       0.685692        0.680971     TDP\n",
      "0       0.249717        0.248733      WT\n",
      "1       0.202228        0.206774     SPE\n",
      "       WT  SPE  IF RNG  IF ARM  TDP  IF ERR  Predictions     ID\n",
      "985   195   40      75      80   80      80    10.996478  74403\n",
      "1287  200   55      75      80   75      65     9.698550  79356\n",
      "757   201   50      70      80   60      75     9.609193  71948\n",
      "253   200   65      75      75   70      75     9.505054  65592\n",
      "821   185   65      70      80   70      70     9.212634  72869\n",
      "699   201   35      65      80   70      65     7.942207  71469\n",
      "1255  182   70      70      75   65      70     7.919384  77891\n",
      "333   203   35      75      70   70      70     7.909578  66581\n",
      "635   200   65      65      75   60      65     7.119540  70874\n",
      "71    201   50      55      80   70      75     7.089433  62151\n",
      "1283  199   30      60      75   75      80     7.007497  79266\n",
      "1122  202   45      65      75   65      65     6.910821  75555\n",
      "377   201   45      65      70   65      75     6.758184  67362\n",
      "599   197   30      70      75   60      65     6.739787  70529\n",
      "1000  201   60      60      75   65      70     6.553357  74509\n",
      "710   200   60      70      70   75      65     6.318637  71601\n",
      "884   201   70      60      80   45      50     6.316276  73552\n",
      "946   203   20      60      75   35      70     6.226604  73998\n",
      "446   200   50      60      75   75      70     6.139245  68290\n",
      "282   188   40      75      70   60      65     6.051256  65849\n",
      "1213  209   25      55      75   40      65     5.947789  76553\n",
      "459   197   55      65      70   65      70     5.875247  68637\n",
      "276   198   65      60      75   50      60     5.713929  65645\n",
      "120   186   45      60      75   60      70     5.656713  63017\n",
      "498   199   45      65      70   80      80     5.559501  69126\n",
      "421   200   30      60      75   55      65     5.308522  68217\n",
      "30    201   45      60      70   65      65     4.826475  60420\n",
      "390   186   25      55      80   60      60     4.786503  67496\n",
      "522   195   30      55      75   65      65     4.679016  69575\n",
      "743   193   30      65      70   70      60     4.487788  71762\n",
      "523   201   35      55      80   45      50     4.449803  69584\n",
      "307   199   70      65      70   60      55     4.435691  66387\n",
      "495   200   65      60      70   70      65     4.402390  69120\n",
      "1251  190   45      45      80   35      70     4.305936  77590\n",
      "823   201   25      55      75   45      60     4.284791  72878\n",
      "468   199   25      45      80   40      65     4.276089  68725\n",
      "506   184   65      75      65   60      65     4.235069  69208\n",
      "1145  177   70      70      75   60      45     4.143361  75688\n",
      "1061  203   30      55      65   35      75     3.827327  75256\n",
      "532   200   40      55      75   50      55     3.784900  69604\n",
      "666   201   35      55      70   50      65     3.758864  70968\n",
      "369   170   60      65      70   65      65     3.751759  66994\n",
      "159   197   35      50      75   55      65     3.748624  63825\n",
      "729   202   35      55      65   50      75     3.592064  71680\n",
      "3     198   40      50      75   65      65     3.555528  57190\n",
      "860   186   65      70      65   60      65     3.363227  73175\n",
      "919   192   55      75      65   65      60     3.336539  73635\n",
      "557   202   50      45      80   50      55     3.226630  70099\n",
      "1111  203   50      55      65   50      75     3.222445  75483\n",
      "1127  198   30      55      70   65      65     2.884006  75569\n",
      "\n",
      "Training model for position 4\n",
      "Empty DataFrame\n",
      "Columns: [WT, SPE, IF RNG, IF ARM, TDP, IF ERR, runsPAdjSeason]\n",
      "Index: []\n",
      "(7121, 7)\n",
      "Epoch [2000/10000], Best Loss: 31.5140724\n",
      "Epoch [4000/10000], Best Loss: 30.5929012\n",
      "Epoch [6000/10000], Best Loss: 29.9601841\n",
      "Epoch [8000/10000], Best Loss: 29.5772762\n",
      "Epoch [10000/10000], Best Loss: 29.1734543\n",
      "Epoch [9999/10000], Loss: 29.2215805\n",
      "Test Mean Squared Error: 31.8712444\n",
      "   mean_abs_shap  stdev_abs_shap    name\n",
      "2      17.893923        9.017537  IF RNG\n",
      "3       1.924149        1.673227  IF ARM\n",
      "4       1.830119        1.765963     TDP\n",
      "5       1.504118        1.357222  IF ERR\n",
      "1       1.020681        1.047163     SPE\n",
      "0       0.821684        1.011027      WT\n",
      "       WT  SPE  IF RNG  IF ARM  TDP  IF ERR  Predictions     ID\n",
      "1154  163   50      75      70   80      80    31.153597  75826\n",
      "696   169   45      75      60   70      70    26.994642  71451\n",
      "1286  200   55      75      80   75      65    24.637571  79356\n",
      "332   203   35      75      70   70      70    23.778164  66581\n",
      "1277  185   50      75      70   70      70    22.870922  78930\n",
      "1258  202   40      75      65   60      70    22.754765  77905\n",
      "281   188   40      75      70   60      65    21.496422  65849\n",
      "1222  169   45      65      60   70      70    21.175198  76627\n",
      "527   200   40      65      80   70      75    20.644917  69592\n",
      "709   200   60      70      70   75      65    20.219601  71601\n",
      "368   170   60      65      70   65      65    19.920616  66994\n",
      "598   197   30      70      75   60      65    19.864244  70529\n",
      "918   192   55      75      65   65      60    19.801521  73635\n",
      "1144  177   70      70      75   60      45    19.779011  75688\n",
      "324   202   75      65      80   60      65    18.688057  66536\n",
      "859   186   65      70      65   60      65    18.452999  73175\n",
      "766   203   40      70      55   65      60    18.153841  72286\n",
      "1236  174   50      65      65   60      55    17.609610  76879\n",
      "418   201   60      65      70   65      70    17.531548  68214\n",
      "731   198   65      65      65   75      70    17.265266  71685\n",
      "634   200   65      65      75   60      65    17.126354  70874\n",
      "458   197   55      65      70   65      70    16.987091  68637\n",
      "479   201   60      70      60   65      50    16.952684  68919\n",
      "1160  196   60      65      65   50      70    16.631432  75846\n",
      "733   204   60      65      75   60      60    16.527382  71723\n",
      "464   196   60      65      75   60      65    16.472088  68711\n",
      "995   200   45      65      75   65      60    16.191391  74437\n",
      "1168  181   65      65      60   70      60    16.108248  75958\n",
      "445   200   50      60      75   75      70    15.649114  68290\n",
      "305   200   40      60      65   75      80    15.178535  66343\n",
      "676   200   45      65      60   50      65    15.131654  71329\n",
      "1233  203   30      65      45   55      60    14.985690  76861\n",
      "1008  200   65      65      65   65      55    14.923588  74676\n",
      "576   184   50      60      80   65      60    14.331598  70332\n",
      "1239  198   60      60      70   70      80    14.268747  76909\n",
      "396   201   55      65      45   60      55    14.092030  67629\n",
      "119   186   45      60      75   60      70    14.035172  63017\n",
      "836   201   50      65      45   50      55    13.966639  72987\n",
      "385   200   35      60      65   70      70    13.739912  67383\n",
      "1112  193   40      65      70   60      55    13.708329  75486\n",
      "29    201   45      60      70   65      65    13.389042  60420\n",
      "98    191   45      65      60   55      50    12.864047  62958\n",
      "1148  181   55      60      60   60      60    12.273970  75784\n",
      "494   200   65      60      70   70      65    11.739094  69120\n",
      "680   190   55      60      80   65      50    11.388029  71336\n",
      "52    190   30      60      60   70      55    11.142747  61198\n",
      "277   198   50      60      70   65      55    10.893361  65647\n",
      "435   201   60      60      60   55      60    10.559294  68276\n",
      "704   188   45      60      60   60      60    10.433096  71565\n",
      "275   198   65      60      75   50      60     6.508255  65645\n",
      "\n",
      "Training model for position 3\n",
      "Empty DataFrame\n",
      "Columns: [WT, SPE, IF RNG, IF ARM, TDP, IF ERR, HT, runsPAdjSeason]\n",
      "Index: []\n",
      "(7605, 8)\n",
      "Epoch [2000/10000], Best Loss: 16.7218609\n",
      "Epoch [4000/10000], Best Loss: 16.3600864\n",
      "Epoch [6000/10000], Best Loss: 16.1533108\n",
      "Epoch [8000/10000], Best Loss: 16.0234165\n",
      "Epoch [10000/10000], Best Loss: 15.8900175\n",
      "Epoch [9999/10000], Loss: 15.8906345\n",
      "Test Mean Squared Error: 16.4729633\n",
      "   mean_abs_shap  stdev_abs_shap    name\n",
      "2       2.856480        1.902505  IF RNG\n",
      "6       0.708324        0.648058      HT\n",
      "5       0.573250        0.504696  IF ERR\n",
      "3       0.311597        0.283167  IF ARM\n",
      "1       0.250237        0.231422     SPE\n",
      "0       0.214968        0.304927      WT\n",
      "4       0.165811        0.160245     TDP\n",
      "       WT  SPE  IF RNG  IF ARM  TDP  IF ERR   HT  Predictions     ID\n",
      "413   203   35      65      65   50      55  197     8.581203  67917\n",
      "405   200   55      60      40   60      45  196     7.361387  67829\n",
      "54    200   45      55      50   55      45  201     7.348724  61220\n",
      "274   198   65      60      75   50      60  195     7.274889  65645\n",
      "1197  202   75      60      45   25      50  191     7.235004  76270\n",
      "472   200   30      60      70   55      55  193     7.111019  68858\n",
      "1015  202   55      55      45   25      50  199     6.948089  74861\n",
      "944   203   20      60      75   35      70  188     6.877239  73998\n",
      "916   199   75      55      45   55      55  196     6.796512  73628\n",
      "1081  202   55      60      65   30      35  192     6.698514  75349\n",
      "359   204   30      45      45   55      45  204     6.651013  66944\n",
      "192   203   45      55      40   55      50  195     6.268923  64370\n",
      "236   201   55      60      50   60      55  189     6.020227  65143\n",
      "215   199   55      50      35   60      50  198     5.851512  65023\n",
      "1217  200   65      60      50   50      65  187     5.808430  76590\n",
      "690   203   35      55      70   40      50  189     5.577122  71378\n",
      "219   200   35      50      55   70      60  192     5.489046  65028\n",
      "864   203   65      50      35   55      50  194     5.445543  73225\n",
      "975   203   45      50      45   55      60  194     5.433067  74334\n",
      "137   202   40      50      55   45      50  196     5.420096  63403\n",
      "893   200   30      50      60   50      60  194     5.282490  73579\n",
      "262   199   55      55      65   60      65  187     5.255769  65623\n",
      "952   200   35      55      40   45      50  193     5.158905  74037\n",
      "1094  183   65      60      50   55      55  185     5.054675  75395\n",
      "382   197   30      50      70   40      50  194     5.052297  67378\n",
      "297   197   70      55      65   35      45  188     5.028115  66179\n",
      "551   200   45      45      40   25      45  200     4.703606  70026\n",
      "139   200   50      45      60   30      60  194     4.611579  63416\n",
      "619   198   55      50      50   20      40  193     4.583481  70591\n",
      "801   202   40      45      70   40      50  193     4.290078  72709\n",
      "751   200   30      45      60   45      50  196     4.284309  71903\n",
      "650   198   30      55      30   30      40  188     4.165568  70930\n",
      "1065  200   70      50      40   50      50  186     4.160440  75300\n",
      "887   199   50      50      70   35      45  189     4.128370  73563\n",
      "782   203   25      50      60   35      60  186     4.122669  72314\n",
      "889   192   35      55      55   65      65  186     4.121290  73568\n",
      "241   198   45      55      40   50      60  187     4.086402  65172\n",
      "112   201   20      50      70   40      50  186     4.075694  63007\n",
      "108   190   40      55      55   65      55  187     4.061319  62993\n",
      "1272  180   60      55      45   55      50  186     4.020358  78230\n",
      "222   203   35      50      50   50      50  188     4.019132  65038\n",
      "300   202   60      45      50   60      65  186     3.766547  66181\n",
      "1035  200   35      50      30   25      35  189     3.680503  74907\n",
      "47    197   55      55      40   60      50  183     3.674533  61103\n",
      "81    204   20      50      60   50      50  184     3.515763  62301\n",
      "510   199   45      45      60   40      40  195     3.452491  69244\n",
      "844   200   45      45      60   40      45  192     3.299146  73012\n",
      "1164  188   60      50      50   50      55  186     3.188254  75929\n",
      "104   174   35      55      50   55      55  184     3.034662  62975\n",
      "1151  187   55      50      60   50      45  189     2.929822  75800\n",
      "\n",
      "Training model for position 2\n",
      "Empty DataFrame\n",
      "Columns: [WT, SPE, C ABI, C ARM, C FRM, runsPAdjSeason]\n",
      "Index: []\n",
      "(4062, 6)\n",
      "Epoch [2000/10000], Best Loss: 1.3916879\n",
      "Epoch [4000/10000], Best Loss: 1.3183305\n",
      "Epoch [6000/10000], Best Loss: 1.2761861\n",
      "Epoch [8000/10000], Best Loss: 1.2464521\n",
      "Epoch [10000/10000], Best Loss: 1.2246597\n",
      "Epoch [9999/10000], Loss: 1.2329147\n",
      "Test Mean Squared Error: 1.5611737\n",
      "   mean_abs_shap  stdev_abs_shap   name\n",
      "2       0.596344        0.456804  C ABI\n",
      "3       0.173987        0.165402  C ARM\n",
      "4       0.082829        0.095074  C FRM\n",
      "1       0.060170        0.086774    SPE\n",
      "0       0.039281        0.055633     WT\n",
      "       WT  SPE  C ABI  C ARM  C FRM  Predictions     ID\n",
      "1253  192   20     70     40     60     1.820420  77651\n",
      "313   189   20     80     50     70     1.496982  66408\n",
      "515   200   25     70     45     65     1.445275  69246\n",
      "331   201   30     75     45     65     1.425668  66574\n",
      "936   214   35     70     60     70     1.386738  73909\n",
      "693   217   20     70     45     65     1.365808  71394\n",
      "937   198   20     80     50     75     1.310638  73917\n",
      "503   184   20     70     55     70     1.293888  69189\n",
      "858   201   25     75     50     70     1.286049  73172\n",
      "761   201   25     75     50     70     1.286049  72260\n",
      "735   200   35     75     50     65     1.199588  71740\n",
      "702   203   20     75     55     65     1.182493  71509\n",
      "778   205   25     70     55     70     1.182197  72301\n",
      "677   201   25     75     55     65     1.146521  71332\n",
      "671   203   25     70     55     65     1.133868  71152\n",
      "964   200   35     70     55     60     1.131711  74113\n",
      "402   200   25     70     55     65     1.123996  67682\n",
      "420   196   20     65     55     75     1.076441  68216\n",
      "794   202   30     80     55     75     1.020852  72632\n",
      "404   201   20     75     60     65     1.012049  67784\n",
      "826   200   25     70     60     65     1.002589  72958\n",
      "843   203   25     70     60     65     0.995553  73008\n",
      "337   201   25     70     60     65     0.988962  66885\n",
      "1088  196   25     65     55     70     0.972362  75361\n",
      "1055  195   35     70     55     65     0.968032  74967\n",
      "401   200   35     80     55     75     0.933823  67664\n",
      "1106  197   35     75     65     65     0.918353  75432\n",
      "426   202   20     70     60     65     0.917332  68244\n",
      "597   203   20     75     65     75     0.893870  70523\n",
      "336   197   40     75     60     70     0.855006  66880\n",
      "871   195   20     65     60     70     0.840917  73310\n",
      "670   202   35     70     60     75     0.832704  70980\n",
      "753   202   20     70     65     60     0.824599  71911\n",
      "828   200   30     80     65     80     0.781889  72976\n",
      "938   199   35     75     70     70     0.772115  73942\n",
      "1269  201   30     70     65     70     0.769247  78100\n",
      "470   200   35     70     65     70     0.757175  68853\n",
      "252   201   20     70     65     70     0.744439  65557\n",
      "461   202   35     75     75     75     0.720111  68687\n",
      "319   200   40     70     65     70     0.717523  66442\n",
      "713   202   30     65     60     65     0.713171  71654\n",
      "534   199   20     65     70     45     0.708853  69623\n",
      "1192  196   35     75     70     70     0.703088  76257\n",
      "630   199   20     70     70     75     0.701733  70869\n",
      "827   202   20     70     70     70     0.677037  72975\n",
      "892   200   20     70     70     65     0.651554  73571\n",
      "1249  204   30     65     55     60     0.623553  77559\n",
      "1036  200   30     65     55     60     0.588945  74920\n",
      "403   201   20     65     50     60     0.584628  67723\n",
      "173   200   20     65     60     65     0.535228  64250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "for position in positions:\n",
    "    print(f\"\\r\\nTraining model for position {position}\")\n",
    "    fieldingModel = FieldingModel(league, season_start, season_end, position, ratings_type)\n",
    "    fieldingModel.load_data(ip_limit)\n",
    "\n",
    "    epoch_count, loss = fieldingModel.train(epochs)\n",
    "    print(f\"Epoch [{epoch_count}/{epochs}], Loss: {loss:.7f}\")\n",
    "\n",
    "    test_loss = fieldingModel.evaluate()\n",
    "    print(f\"Test Mean Squared Error: {test_loss:.7f}\")\n",
    "\n",
    "    feature_importance = fieldingModel.feature_importance()\n",
    "    print(feature_importance)\n",
    "\n",
    "    fieldingModel.save_model()\n",
    "\n",
    "    predictions = fieldingModel.predict(season_end, ip_limit)\n",
    "    predictions.sort_values(by='Predictions', ascending=False, inplace=True)\n",
    "    print(predictions.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
